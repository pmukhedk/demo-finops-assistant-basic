import streamlit as st
import os
from dotenv import load_dotenv
from utils import extract_text

# Load environment variables
load_dotenv()

# Qdrant
from qdrant_client import QdrantClient
from qdrant_client.http.models import VectorParams, Distance

# LangChain
from langchain_qdrant import QdrantVectorStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings

# OpenAI-compatible client (vLLM)
from openai import OpenAI

# ---------------------------------------------------
# CONFIGURATION
# ---------------------------------------------------
# QDRANT
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME", "rag_docs")

# EMBEDDINGS
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
EMBEDDING_DIM = 384  # Matches all-MiniLM-L6-v2

# vLLM (OpenAI-compatible)
VLLM_BASE_URL = os.getenv("VLLM_BASE_URL")
VLLM_MODEL_NAME = os.getenv("VLLM_MODEL_NAME")
VLLM_API_KEY = os.getenv("VLLM_API_KEY")  # vLLM ignores this but OpenAI client requires it

# ---------------------------------------------------
# STREAMLIT PAGE SETUP
# ---------------------------------------------------
st.set_page_config(
    page_title="VOIS FinOps AI Assistant : Prototype",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Hide sidebar collapse button to create a "fixed" layout AND apply custom font
st.markdown(
    """
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap');

    html, body, [class*="css"] {
        font-family: 'Inter', sans-serif;
    }
    
    [data-testid="stSidebarCollapse"] {
        display: none;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ---------------------------------------------------
# AUTHENTICATION
# ---------------------------------------------------
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.title("üîê Login")
    password = st.text_input("Enter Password", type="password")
    if st.button("Login"):
        if password == os.getenv("APP_PASSWORD"):
            st.session_state.authenticated = True
            st.rerun()
        else:
            st.error("Incorrect password")
    st.stop()

st.title("VOIS FinOps AI Assistant : Prototype")

with st.expander("‚öñÔ∏è Disclaimer and Data Handling Information", expanded=False):
    st.markdown("""
    This AI Assistant prototype provides analytical insights derived exclusively from cloud billing data voluntarily uploaded by the user, including but not limited to AWS Cost and Usage Reports, Azure cost exports, and CSV billing files. The application does not establish connections to live cloud environments, does not access cloud provider accounts, and does not perform any automated or manual actions on customer infrastructure.

    **Data Handling and Confidentiality**
    This FinOps AI Assistant is deployed within secure environment and utilizes open-weight large language models on Red Hat AI as the foundational platform. The architecture ensures full organizational control over infrastructure, networking, identity and access management, data residency, and operational governance, supporting enterprise security and regulatory compliance requirements.

    **Limitations and Disclaimer**
    Insights, forecasts, and optimization recommendations generated by the VOIS FinOps AI Assistant : Prototype are provided for informational purposes only. Such outputs do not constitute financial, operational, or professional advice. The accuracy and completeness of the results depend on the quality, structure, and scope of the data supplied by the user. Users are solely responsible for validating and approving any actions taken based on the generated insights.

    **Product Maturity**
    This application is provided as an evolving product. Features, analytical methodologies, and outputs may be modified, enhanced, or discontinued without prior notice as part of ongoing development and improvement efforts.
    """)

st.markdown("""
Upload supported billing data sources, including AWS Cost and Usage Reports, Azure Cost Exports, and CSV files, to enable detailed cost analysis aligned with FinOps principles and natural language‚Äìbased insight discovery.
""")

# ---------------------------------------------------
# INITIALIZE CLIENTS (WITH CACHING)
# ---------------------------------------------------
@st.cache_resource
def get_qdrant_client():
    return QdrantClient(
        url=QDRANT_URL,
        api_key=QDRANT_API_KEY,
        check_compatibility=False
    )

@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL
    )

@st.cache_resource
def get_llm_client():
    return OpenAI(
        base_url=VLLM_BASE_URL,
        api_key=VLLM_API_KEY
    )

qdrant_client = get_qdrant_client()
embeddings = get_embeddings()
llm_client = get_llm_client()

# ---------------------------------------------------
# ENSURE COLLECTION EXISTS (Cached Check)
# ---------------------------------------------------
@st.cache_resource
def ensure_collection_exists(_client, collection_name):
    existing_collections = [
        c.name for c in _client.get_collections().collections
    ]

    if collection_name not in existing_collections:
        _client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(
                size=EMBEDDING_DIM,
                distance=Distance.COSINE
            )
        )

ensure_collection_exists(qdrant_client, COLLECTION_NAME)

# ---------------------------------------------------
# VECTOR STORE
# ---------------------------------------------------
vectorstore = QdrantVectorStore(
    client=qdrant_client,
    collection_name=COLLECTION_NAME,
    embedding=embeddings
)

# ---------------------------------------------------
# vLLM CLIENT (OPENAI API COMPATIBLE)
# ---------------------------------------------------
# (Client initialized above with caching)

# ---------------------------------------------------
# SIDEBAR: FILE UPLOAD
# ---------------------------------------------------
st.sidebar.header("üìÇ Data Ingestion")
st.sidebar.markdown("Upload **Billing Data** (CSV/Excel) or **Policy PDFs**.")

uploaded_files = st.sidebar.file_uploader(
    "Upload Files",
    type=["pdf", "txt", "csv", "xlsx"],
    accept_multiple_files=True
)

st.sidebar.caption("Powered by Red Hat AI")

if uploaded_files:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )

    for file in uploaded_files:
        # Initialize session state for this file if not present
        if f"processed_{file.name}" not in st.session_state:
            with st.spinner(f"Processing {file.name}..."):
                text, df = extract_text(file)

                if not text:
                    st.sidebar.warning(f"‚ùå Unsupported or empty file: {file.name}")
                    continue
                
                if text.startswith("Error"):
                     st.sidebar.error(text)
                     continue

                # Ingest into Vector Store
                docs = splitter.create_documents([text])
                vectorstore.add_documents(docs)

                st.sidebar.success(f"‚úÖ Ingested: {file.name}")
                
                # --- Automatic FinOps Analysis ---
                if df is not None:
                    with st.spinner("ü§ñ Generating FinOps Health Assessment..."):
                        # Create a condensed summary for the analysis prompt
                        analysis_context = text[:15000] # Limit context to avoid token overflow
                        
                        analysis_prompt = f"""
                        You are a Senior FinOps Auditor. 
                        Analyze the following cloud billing summary data and provide a "FinOps Health Assessment".
                        
                        DATA SUMMARY:
                        {analysis_context}
                        
                        STRICT OUTPUT STRUCTURE:
                        
                        1. Overall Consumption Pattern (What the data says)
                           - Identify top cost drivers (e.g., EC2, EBS).
                           - Analyze trends (spikes, flatlines, or drops).
                           - Identify anomalies (zero usage, detached volumes).
                        
                        2. What This Tells About Usage Maturity
                           - Provide a table with columns: Area | Maturity Level (High/Medium/Low)
                           - Areas to score: Provisioning, Rightsizing, Storage Governance, Commitment.
                        
                        3. Recommended Consumption Pattern
                           - Specific actionable advice (e.g., "Move from On-Demand to Savings Plans").
                           - Quantify expected savings if possible (e.g., "20-30%").
                        
                        TONE: Professional, critical, and directive. Use emojis (üîπ, üìå, ‚úÖ, ‚ö†Ô∏è, ‚ùå) for readability.
                        """
                        
                        response = llm_client.chat.completions.create(
                            model=VLLM_MODEL_NAME,
                            messages=[{"role": "user", "content": analysis_prompt}],
                            temperature=0.3,
                            max_tokens=1500
                        )
                        
                        # Store analysis in session state
                        st.session_state[f"analysis_{file.name}"] = response.choices[0].message.content
            
            # Mark as processed
            st.session_state[f"processed_{file.name}"] = True

    # Display Analysis from Session State (so it persists without re-running LLM)
    analysis_exists = False
    for file in uploaded_files:
        if f"analysis_{file.name}" in st.session_state:
            analysis_exists = True
            st.markdown("---")
            st.subheader(f"üìä FinOps Analysis: {file.name}")
            st.markdown(st.session_state[f"analysis_{file.name}"])

# ---------------------------------------------------
# MAIN: QUESTION ANSWERING (RAG) - Only show if analysis exists
# ---------------------------------------------------
if 'analysis_exists' in locals() and analysis_exists:
    st.markdown("---")
    st.subheader("üí¨ Ask AI FinOps Assistant")

    question = st.text_input("Example: 'Why did EC2 costs spike last month?' or 'Show top 5 idle resources.'")

    if question:
        with st.spinner("üîç Analyzing cost data & policies..."):
            # Increased k to 10 to ensure we catch the 'Waste' chunks which might be further down
            docs = vectorstore.similarity_search(question, k=10)

        if not docs:
            st.warning("No relevant billing data or policies found.")
        else:
            context = "\n\n".join([doc.page_content for doc in docs])
            
            # Debugging: Show what the LLM actually sees
            with st.expander("üïµÔ∏è Debug: What the AI sees (Context)"):
                st.text(context)

            prompt = f"""
You are a Senior FinOps Auditor.
Use ONLY the provided document context below to answer the user's question. 
Act as if you are "talking to the document".

**DEFINITION OF IDLE/WASTE:**
For this analysis, consider "Idle" or "Wasted" resources to include:
- Resources explicitly listed in the "POTENTIAL IDLE / WASTED RESOURCES" section.
- Snapshots, Unattached Volumes, or Unused IPs.
- Resources with very low recurring costs (<$5) that may be forgotten debris.
- You do NOT need CPU/Memory metrics to identify these candidates; use the cost/type evidence provided.

DO NOT fabricate numbers, resource IDs, or costs.
If the context really contains NO mentions of specific resources or waste candidates, strictly state: "No relevant data found in the provided documents to answer this."

Context (Billing Data/Policies):
{context}

Question:
{question}

Response Format:
1. **Analysis:** Direct answer based on the document data.
2. **Evidence:** Cite specific numbers, dates, or resource IDs from the context.
3. **Recommendation:** Actionable advice based on FinOps principles.
"""

            response = llm_client.chat.completions.create(
                model=VLLM_MODEL_NAME,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=1024
            )

            st.markdown("### üí° FinOps Insight")
            st.write(response.choices[0].message.content)

            with st.expander("üìä Source Data (Context)"):
                st.code(context)

